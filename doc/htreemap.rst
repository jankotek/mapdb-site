HTreeMap
========

HTreeMap provides ``HashMap`` and ``HashSet``. 
It has great performance with large keys. 
It also offers entry expiration based on size or time-to-live

HTreeMap is *segmented Hash Tree*. Most hash collections use hash table based in fixed size array, 
when it becomes full, all data has to be moved and rehashed into 
new bigger table. HTreeMap uses auto-expandind Hash Treestructure, so it never needs resizing. 
It also occupies less space, since empty hash slots do not consume any space.
On other side tree structure requries more seeks and is slower on access. 
Its performance degrades with size, but maximal has dir node size is 255,
so degradation is very small. 
TODO performance degradation depending on size. Probably  log N.

To achieve parallel scalability HTreeMap is split into 16  segments,
each with separate read-write lock. ``ConcurrentHashMap`` in JDK 7 works similar
way. Number of segments (also called concurrency scale) is hard wired
into design and can not be changed.
Because all segments share underlying storage, concurrent scalability is not perfect.
Other option is to create each segment with separate storage. 

HTreeMap optionally supports entry expiration based on four criteria:
maximal map size, time-to-live since last modification and time-to-live
since last access. Expired entries are automatically removed. This
feature uses FIFO queue, each segment has independent expiration queue.
Priority per entry can not be set.

Parameters
----------

HTreeMap has number of parameters to tune its performance. Number of
segments (aka concurrency factor) is hard-coded to16 and can not be
changed. Other params can be set only when map is created and can not be
changed latter.

Most important are **serializers**. General serialization has
some guessing and overhead, so it always has better performance to use
more specific serializers. To specify key and value serializer use code
bellow. There are dozens ready to use serializers available as static
fields on ``Serializer`` interface:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_serializer.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

HTreeMap is recommended for handling large key/values. In same cases you
may want to use compression. Enabling compression store-wide is notr
always best, since constantly (de)compressing index tree has overhead.
Instead it is better to apply compression just to specific serializer on key or value.
This is done by using serializer wrapper:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_compressed.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8
            
Most hash maps uses 32bit hash generated by ``Object.hashCode()`` and check equality with ``Object.equals(other)``. 
However many classes do not implement those functions correctly, and inconsistent hashing is 
very bad for persistence, it could cause data loss.
In default configuration HTreeMap uses generic key serializer and relies on those methods as well,
however it throws an ``IllegalArgumentException``, if inconsistent hashing is detected (for example ``byte[]``
is used without serializer).

If specialized Key Serializer is defined, HTreeMap relies on it to provide hash code and equality check 
for keys. For example ``Serializer.BYTE_ARRAY`` uses  ``java.util.Arrays.hashCode(byte[])`` to generate hash code.
This way you can use primitive arrays directly as a key/value without a wrapper. 
Bypassing wrappers such as ``String`` improves performance: 

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_byte_array.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8


Other parameter is **size counter**. By default HTreeMap does not keep
track of its size, calling ``map.size()`` requires linear scan to count
all entries. You can enable size counter, in that case
``map.size()`` is instant, but there is some overhead on inserts.

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_counter.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

And finally some sugar. There is **value creator**, a function to create
value if existing value is not found. Newly created value is inserted
into map. This way ``map.get(key)`` never returns null. This is mainly
useful for various generators and caches.

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_value_creator.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8


or more readable version in Java 8:

.. code:: java

        HTreeMap<String,Long> map = db.hashMapCreate("map")
                .valueCreator((key)-> 1111L)
                .makeOrGet();

        // this way map.get() returns 1111L if no value is found
        map.get("aa"); // 1111L
        map.get("bb"); // 1111L

        // map now contains ["aa"->1111L, "bb"->1111L]

Entry expiration parameters
---------------------------

``HTreeMap`` offers optional entry expiration if some conditions are
met. Entry can expire if:

-  Number of entries in map would exceed maximal size

-  Entry exist in map longer time than expiration period is. The
   expiration period could be since last modification or since last read
   access.

-  Disk/memory space consumed by Map is bigger then some limit in GB.

There is shortcut in ``DBMaker`` to quickly use ``HTreeMap`` as off-heap
cache with memory size limit:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_cache_space_limit.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

This equals to ``expireStoreSize`` param:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_cache_space_limit2.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

It is also possible to limit maximal size of map:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_cache_size_limit.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

And finally you can set expiration time since last modification or since
last access.

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_cache_ttl_limit.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

TODO expiration counts are approximate. Map size can go slightly over limits for short period of time.

TODO disk space limit has issues. Investigate how it works and document

TODO expiration threads single and multithreaded.

Binding and overflow
-----------------------

Maps in MapDB supports call back notification on insert, update and removal.
There is ``Bind`` class (TODO link to chapter) which links two collections together and keeps them synchronized.
There are several ways to associate collections, it is described in separate chapter (TODO link).

Specific for ``HTreeMap`` is expiration overflow. For cache one can keep most recently used
entries in memory for faster access. After entry expires it is moved to slower storage on disk.

To create overflow you need two maps. One on-disk and one in-memory. You bind them together with
``.expireOverflow(onDisk, true)`` parameter in builder:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_overflow_init.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

Once binding is established, every entry removed from ``inMemory`` map will be added to ``onDisk`` map.
This applies to expired entries, but also entries removed using ``map.remove()`` method.
To completely remove entry from both maps one must first remove from ``inMemory`` then from ``onDisk``:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_overflow_remove.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

If ``inMemory.get(key)`` is called, one might expect null, and than check ``onDisk``. However to make things simpler
``inMemory`` has value creator. So ``inMemory.get(key)`` returns values from both ``inMemory`` and ``onDisk``.
If value is not found, it checks second map and adds its value into ``inMemory``.

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_overflow_get.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

Get and removal synchronizes two collections in cyclic way. Entry gets moved into ``onDisk`` when its deleted from ``inMemory``.
And ``inMemory`` gets populated from ``onDisk`` when get does find value. This creates consistency problem,
which map is authoritative? Which contains the 'real' data?

MapDB offers three alternatives. Method ``.expireOverflow(onDisk, true)`` takes boolean parameter to control
how expiration behaves. With ``false`` expiration uses ``map.put(key,value)`` to insert data to inDisk.
With ``true`` it uses ``map.putIfAbsent(key,value``, so no existing value gets overwritten.

First option is ``true``. It mean that ``inMemory`` is authoritative and ``onDisk`` content will get
overwritten once data expires from memory and overflows to disk:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_overflow_main_inmemory.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

With ``false`` the data added into ``inMemory`` are not considered authoritative. OnDisk content will never get
overwritten. This also adds performance bonus on expiration if values are the same, since on-disk values
are not overwritten by equal values.

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_overflow_main_ondisk.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

There is a question into which collection new data should be inserted. It depends how much you value your data.
If it caches some external source (such as SQL), I would insert data into ``inMemory`` and let them hoover in air.
In this case data might be inconsistent after crash/shutdown. It is better to drop cache content and
repopulate it from primary store.

If content of cache is valuable (is primary content, or too high cost to repopulate from SQL)
one should insert data to ``onDisk``.
Disk store can be protected by transaction with periodic commit.
In this case only small time interval of data would be lost.

Inserts are simple, but updates create consistency problem. If key-value pair changes one collection might contain
older value. So on updates it is recommended to update BOTH ``inMemory`` and ``onDisk`` collections:

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_overflow_update.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8



Concurrent scalability
------------------------

HTreeMap scales concurrently by using 16 separate segment, each with its own ``ReadWriteLock``. 
Each segment has its own independent state, hash tree and also expiration queue.
But all segments still share underlying storage and are limited by its performance. 

There is option to shard HTreeMap. Each separate segment can get its own storage, so no shared
state exist between segments. This way one get linear concurrent scalability which corresponds
to 16 segments. TODO benchmarks.

Trade off in this case is higher memory consumption. There are 16 different stores, each with its own
memory allocator and unused blocks. TODO memory benchmarks. 
But each store can be compacted separately. TODO add compaction doc for this 

.. literalinclude:: ../../mapdb/src/test/java/doc/htreemap_segmented.java
    :start-after: //a
    :end-before: //z
    :language: java
    :dedent: 8

Compared to BTreeMap
--------------------

`HTreeMap` has major advantage over `BTreeMap` with large keys. Unlike
BTreeMap it only stores hash codes in tree nodes. 
BTreeMap deserializes tree nodes together with their keys on each lookup. 
Simple ``BTreeMap.get(key)`` could deserialize houndreds of keys. 

TODO link to performance test, compare with BTreeMap

On other side HTreeMap has limited concurrency factor to 16, so with  
writes it wont scale over 4 CPU cores. It uses read-write locks, so read
operations are not affected. However in practice disk IO is more likely
to be bottleneck. TODO benchmarks

HTreeMap can be easily sharded by segments. For in-memory map it might have better 
concurrent scalability.

HTreeMap is simpler than BTreeMap. It has more predictable performance over long time
and does not get fragmented after frequent deletes.
HTreeMap also offers expiration. 
BTreeMap pays tax in some cases for its complex lock-free design.
